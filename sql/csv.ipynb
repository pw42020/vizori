{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from typing_extensions import TypedDict\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "path_to_csv: str = f\"../data/train.csv\"\n",
    "\n",
    "@dataclass\n",
    "class Insightly:\n",
    "    \"\"\"\n",
    "    A class to represent a connection to a DuckDB database.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    conn : duckdb.DuckDBPyConnection\n",
    "        The DuckDB connection object.\n",
    "    \"\"\"\n",
    "    conn: duckdb.DuckDBPyConnection = field(default_factory=duckdb.connect)\n",
    "    \n",
    "    # reading the CSV file into a DuckDB table\n",
    "    def read_csv_to_duckdb(\n",
    "        self,\n",
    "        path_to_csv: str,\n",
    "        table_name: str = \"titanic\"\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Reads a CSV file into a DuckDB table.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        conn : duckdb.DuckDBPyConnection\n",
    "            The DuckDB connection object.\n",
    "        path_to_csv : str\n",
    "            The path to the CSV file.\n",
    "        table_name : str, optional\n",
    "            The name of the table to create in DuckDB (default is \"titanic\").\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.conn.execute(\n",
    "            f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} AS\n",
    "            SELECT * FROM read_csv_auto('{path_to_csv}')\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def retrieve_table(\n",
    "        self,\n",
    "        table_name: str = \"titanic\"\n",
    "    ) -> duckdb.DuckDBPyRelation:\n",
    "        \"\"\"\n",
    "        Retrieves a DuckDB table as a relation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        conn : duckdb.DuckDBPyConnection\n",
    "            The DuckDB connection object.\n",
    "        table_name : str, optional\n",
    "            The name of the table to retrieve (default is \"titanic\").\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        duckdb.DuckDBPyRelation\n",
    "            The relation representing the DuckDB table.\n",
    "        \"\"\"\n",
    "        return self.conn.table(table_name)\n",
    "\n",
    "    # get the schema using duckdb\n",
    "    def get_schema(self) -> Dict[str, Any]:\n",
    "        schema = self.conn.execute(\n",
    "            f\"\"\"\n",
    "            SELECT column_name, data_type\n",
    "            FROM information_schema.columns\n",
    "            \"\"\"\n",
    "        ).df()\n",
    "        \n",
    "        return str(schema)\n",
    "    \n",
    "    def execute_query(\n",
    "        self,\n",
    "        query: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes a SQL query on the DuckDB connection.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            The SQL query to execute\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            The result of the executed query as a relation.\n",
    "        \"\"\"\n",
    "        return self.conn.sql(query)\n",
    "    \n",
    "# Example usage\n",
    "insightly = Insightly()\n",
    "insightly.read_csv_to_duckdb(path_to_csv)\n",
    "table = insightly.retrieve_table()\n",
    "# print(table.df())\n",
    "schema = insightly.get_schema()\n",
    "print(schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from enum import Enum, auto\n",
    "\n",
    "from pydantic import Field, BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "class PlotType(str, Enum):\n",
    "    \"\"\"Plot type for the visualization of the data.\"\"\"\n",
    "    SCATTER = \"SCATTER\"\n",
    "    # BAR = \"BAR\"\n",
    "    # LINE = \"LINE\"\n",
    "    # HISTOGRAM = \"HISTOGRAM\"\n",
    "    # PIE = \"PIE\"\n",
    "    # HEATMAP = \"HEATMAP\"\n",
    "\n",
    "class SqlQueryInfo(TypedDict):\n",
    "    sql_query: str\n",
    "    query_result: str\n",
    "    query_rows: list\n",
    "    sql_error: bool\n",
    "\n",
    "class PlotQueryInfo(TypedDict):\n",
    "    plot_type: str\n",
    "    columns: list[str]\n",
    "    # plot_query: str\n",
    "    query_result: str\n",
    "    query_rows: list\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    meant_as_query: bool\n",
    "    sql_query_info: Optional[SqlQueryInfo] = None\n",
    "    plot_query_info: Optional[PlotQueryInfo] = None\n",
    "    attempts: int\n",
    "    relevance: str\n",
    "\n",
    "# this is made to be used with the LangChain framework and is a prompt\n",
    "# to ensure that the answer given by the LLM is structured\n",
    "class CheckRelevance(BaseModel):\n",
    "    relevance: str = Field(\n",
    "        description=\"Indicates whether the question is related to the database schema. 'relevant' or 'not_relevant'.\"\n",
    "    )\n",
    "\n",
    "def check_relevance(state: AgentState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    schema = insightly.get_schema()\n",
    "    print(f\"Checking relevance of the question: {question}\")\n",
    "    system = \"\"\"You are an assistant that determines whether a given question is related to the following database schema.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Respond with only \"relevant\" or \"not_relevant\".\n",
    "\"\"\".format(schema=schema)\n",
    "    human = f\"Question: {question}\"\n",
    "    check_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    structured_llm = llm.with_structured_output(CheckRelevance)\n",
    "    relevance_checker = check_prompt | structured_llm\n",
    "    relevance = relevance_checker.invoke({})\n",
    "    state[\"relevance\"] = relevance.relevance\n",
    "    print(f\"Relevance determined: {state['relevance']}\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryType(str, Enum):\n",
    "    SQL = \"sql\"\n",
    "    PLOT = \"plot\"\n",
    "\n",
    "class CheckIfSQLOrPlot(BaseModel):\n",
    "    meant_as_query: QueryType = Field(\n",
    "        description=\"Indicates whether the question requires an SQL query or a plot.\"\n",
    "    )\n",
    "    type_of_plot: PlotType = Field(\n",
    "        description=\"The type of plot to be generated if the question requires a plot.\"\n",
    "    )\n",
    "\n",
    "def check_if_sql_or_plot(state: AgentState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"Checking if the question requires an SQL query or a plot: {question}\")\n",
    "    system = \"\"\"\n",
    "You are an assistant that determines whether a given question requires an SQL query or a plot based on the following schema:\n",
    "{schema}\n",
    "\n",
    "Respond with 'sql' if the question is related to data retrieval or manipulation that can be expressed in SQL,\n",
    "and 'plot' if the question is related to data visualization.\n",
    "If the question is related to data visualization, provide the type of plot as one of the following with no explanation: BAR.\n",
    "\"\"\"\n",
    "    human = f\"Question: {question}\"\n",
    "    check_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    structured_llm = llm.with_structured_output(CheckIfSQLOrPlot)\n",
    "    type_checker = check_prompt | structured_llm\n",
    "    result = type_checker.invoke({})\n",
    "    state[\"meant_as_query\"] = result.meant_as_query\n",
    "    if result.meant_as_query:\n",
    "        state[\"sql_query_info\"] = SqlQueryInfo(\n",
    "            sql_query=\"\",\n",
    "            query_result=\"\",\n",
    "            query_rows=[]\n",
    "        )\n",
    "    else:\n",
    "        state[\"plot_query_info\"] = PlotQueryInfo(\n",
    "            plot_type=result.type_of_plot.value,\n",
    "            query_result=\"\",\n",
    "            query_rows=[]\n",
    "        )\n",
    "    print(f\"Determined type: {state['meant_as_query']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToSQL(BaseModel):\n",
    "    sql_query: str = Field(\n",
    "        description=\"The SQL query generated from the natural language question.\"\n",
    "    )\n",
    "\n",
    "def convert_nl_to_sql(state: AgentState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    schema = insightly.get_schema()\n",
    "    print(f\"Converting question to SQL: {question}\")\n",
    "    system = \"\"\"You are an assistant that converts natural language questions into SQL queries based on the following schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Provide only the SQL query without any explanations. Alias columns appropriately to match the expected keys in the result.\n",
    "\n",
    "For example, alias 'food.name' as 'food_name' and 'food.price' as 'price'.\n",
    "\"\"\".format(schema=schema)\n",
    "    convert_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    structured_llm = llm.with_structured_output(ConvertToSQL)\n",
    "    sql_generator = convert_prompt | structured_llm\n",
    "    result = sql_generator.invoke({\"question\": question})\n",
    "    state[\"sql_query_info\"][\"sql_query\"] = result.sql_query\n",
    "    print(f\"Generated SQL query: {state[\"sql_query_info\"]['sql_query']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(state: AgentState):\n",
    "    sql_query = state[\"sql_query\"].strip()\n",
    "    print(f\"Executing SQL query: {sql_query}\")\n",
    "    try:\n",
    "        result: duckdb.DuckDBPyRelation = insightly.execute_query(sql_query)\n",
    "        if sql_query.lower().startswith(\"select\"):\n",
    "            rows = result.fetchall()\n",
    "            columns = result.keys()\n",
    "            if rows:\n",
    "                header = \", \".join(columns)\n",
    "                state[\"sql_query_info\"][\"query_rows\"] = [dict(zip(columns, row)) for row in rows]\n",
    "                print(f\"Raw SQL Query Result: {state['query_rows']}\")\n",
    "                # Format the result for readability\n",
    "                data = \"; \".join([f\"{row.get('food_name', row.get('name'))} for ${row.get('price', row.get('food_price'))}\" for row in state[\"query_rows\"]])\n",
    "                formatted_result = f\"{header}\\n{data}\"\n",
    "            else:\n",
    "                state[\"query_rows\"] = []\n",
    "                formatted_result = \"No results found.\"\n",
    "            state[\"sql_query_info\"][\"query_result\"] = formatted_result\n",
    "            state[\"sql_query_info\"][\"sql_error\"] = False\n",
    "            print(\"SQL SELECT query executed successfully.\")\n",
    "        else:\n",
    "            result.commit()\n",
    "            state[\"sql_query_info\"][\"query_result\"] = \"The action has been successfully completed.\"\n",
    "            state[\"sql_query_info\"][\"sql_error\"] = False\n",
    "            print(\"SQL command executed successfully.\")\n",
    "    except Exception as e:\n",
    "        state[\"sql_query_info\"][\"query_result\"] = f\"Error executing SQL query: {str(e)}\"\n",
    "        state[\"sql_query_info\"][\"sql_error\"] = True\n",
    "        print(f\"Error executing SQL query: {str(e)}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "class ScatterPlotter(BaseModel):\n",
    "    plot: str = Field(\n",
    "        description=\"The generated scatter plot as a string representation.\"\n",
    "    )\n",
    "\n",
    "def get_scatter_plot_columns(state: AgentState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    schema = insightly.get_schema()\n",
    "    print(f\"Converting question to SQL: {question}\")\n",
    "    system = \"\"\"You are an assistant that chooses the appropriate columns for a scatter plot based on the following schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Provide only the columns to be used in the scatter plot without any explanations.\n",
    "The columns should be suitable for a scatter plot, typically two numerical columns.\n",
    "\"\"\".format(schema=schema)\n",
    "    convert_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    structured_llm = llm.with_structured_output(ConvertToSQL)\n",
    "    sql_generator = convert_prompt | structured_llm\n",
    "    result = sql_generator.invoke({\"question\": question})\n",
    "    state[\"plot_query_info\"][\"columns\"] = result.sql_query.split(\", \")\n",
    "    print(f\"Selected columns for scatter plot: {state['plot_query_info']['columns']}\")\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scatter_plot(state: AgentState, config: RunnableConfig):\n",
    "    columns = state.get(\"columns\", [])\n",
    "    if len(columns) != 2:\n",
    "        raise ValueError(\"Scatter plot requires exactly two columns.\")\n",
    "    \n",
    "    query = f\"SELECT {columns[0]}, {columns[1]} FROM titanic\"\n",
    "    print(f\"Generating scatter plot with query: {query}\")\n",
    "    \n",
    "    # Execute the query and fetch the data\n",
    "    df = insightly.conn.execute(query).df()\n",
    "    \n",
    "    # Generate a scatter plot (this is a placeholder, actual plotting code would go here)\n",
    "    px.scatter(df, x=columns[0], y=columns[1], title=\"Scatter Plot\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate_funny_response(state: AgentState):\n",
    "    print(\"Generating a funny response for an unrelated question.\")\n",
    "    system = \"\"\"You are a charming and funny assistant who responds in a playful manner.\n",
    "    \"\"\"\n",
    "    human_message = \"I can not help with that, but doesn't asking questions make you hungry? You can always order something delicious.\"\n",
    "    funny_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human_message),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(temperature=0.7)\n",
    "    funny_response = funny_prompt | llm | StrOutputParser()\n",
    "    message = funny_response.invoke({})\n",
    "    state[\"sql_query_info\"][\"query_result\"] = message\n",
    "    print(\"Generated funny response.\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewrittenQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The rewritten question.\")\n",
    "\n",
    "def regenerate_query(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    print(\"Regenerating the SQL query by rewriting the question.\")\n",
    "    system = \"\"\"You are an assistant that reformulates an original question to enable more precise SQL queries. Ensure that all necessary details, such as table joins, are preserved to retrieve complete and accurate data.\n",
    "    \"\"\"\n",
    "    rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"Original Question: {question}\\nReformulate the question to enable more precise SQL queries, ensuring all necessary details are preserved.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    structured_llm = llm.with_structured_output(RewrittenQuestion)\n",
    "    rewriter = rewrite_prompt | structured_llm\n",
    "    rewritten = rewriter.invoke({})\n",
    "    state[\"question\"] = rewritten.question\n",
    "    state[\"attempts\"] += 1\n",
    "    print(f\"Rewritten question: {state['question']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class State(str, Enum):\n",
    "    CHECK_RELEVANCE: str = \"check_relevance\"\n",
    "    CHECK_IF_SQL_OR_PLOT: str = \"check_if_sql_or_plot\"\n",
    "    CONVERT_NL_TO_SQL: str = \"convert_nl_to_sql\"\n",
    "    GET_SCATTER_PLOT_COLUMNS: str = \"get_scatter_plot_columns\"\n",
    "    GENERATE_SCATTER_PLOT: str = \"generate_scatter_plot\"\n",
    "    GENERATE_FUNNY_RESPONSE: str = \"generate_funny_response\"\n",
    "    REGENERATE_QUERY: str = \"regenerate_query\"\n",
    "    EXECUTE_SQL: str = \"execute_sql\"\n",
    "    END_MAX_ITERATIONS: str = \"end_max_iterations\"\n",
    "    \n",
    "# ROUTERS\n",
    "def end_max_iterations(state: AgentState):\n",
    "    state[\"query_result\"] = \"Please try again.\"\n",
    "    print(\"Maximum attempts reached. Ending the workflow.\")\n",
    "    return state\n",
    "\n",
    "def relevance_router(state: AgentState) -> State:\n",
    "    if state[\"relevance\"] == \"relevant\":\n",
    "        return State.CHECK_IF_SQL_OR_PLOT\n",
    "    else:\n",
    "        return State.GENERATE_FUNNY_RESPONSE\n",
    "    \n",
    "def type_router(state: AgentState) -> State:\n",
    "    if state[\"query_type\"]:\n",
    "        return State.CONVERT_NL_TO_SQL\n",
    "    else:\n",
    "        return State.GET_SCATTER_PLOT_COLUMNS\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# relevancy checks at the beginning to ensure question is relevant and can be answered\n",
    "workflow.add_node(\n",
    "    State.CHECK_RELEVANCE,\n",
    "    check_relevance,\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    State.CHECK_RELEVANCE,\n",
    "    relevance_router,\n",
    "    {\n",
    "        State.CHECK_IF_SQL_OR_PLOT: State.CHECK_IF_SQL_OR_PLOT,\n",
    "        State.GENERATE_FUNNY_RESPONSE: State.GENERATE_FUNNY_RESPONSE\n",
    "    },\n",
    ")\n",
    "# checking if the question is meant to be answered with SQL statement or a plot\n",
    "workflow.add_node(\n",
    "    State.CHECK_IF_SQL_OR_PLOT,\n",
    "    check_if_sql_or_plot,\n",
    ")\n",
    "# adding conditional edge to go different directions based on query type\n",
    "workflow.add_conditional_edges(\n",
    "    State.CHECK_IF_SQL_OR_PLOT,\n",
    "    type_router,\n",
    "    {\n",
    "        QueryType.SQL: State.CONVERT_NL_TO_SQL,\n",
    "        QueryType.PLOT: State.GET_SCATTER_PLOT_COLUMNS\n",
    "    }\n",
    ")\n",
    "# if the question is meant to be answered with SQL,\n",
    "# convert the natural language question to SQL query\n",
    "workflow.add_node(\n",
    "    State.CONVERT_NL_TO_SQL,\n",
    "    convert_nl_to_sql,\n",
    ")\n",
    "workflow.add_node(\n",
    "    State.EXECUTE_SQL,\n",
    "    execute_sql,\n",
    ")\n",
    "workflow.add_edge(\n",
    "    State.CONVERT_NL_TO_SQL,\n",
    "    State.EXECUTE_SQL\n",
    ")\n",
    "\n",
    "# if the question is relevant but the SQL query is not executed successfully,\n",
    "# regenerate the query by rewriting the question\n",
    "workflow.add_node(\n",
    "    State.REGENERATE_QUERY,\n",
    "    regenerate_query,\n",
    ")\n",
    "workflow.add_edge(\n",
    "    State.REGENERATE_QUERY,\n",
    "    State.CONVERT_NL_TO_SQL\n",
    ")\n",
    "workflow.add_node(\n",
    "    State.END_MAX_ITERATIONS,\n",
    "    end_max_iterations\n",
    ")\n",
    "workflow.add_edge(\n",
    "    State.END_MAX_ITERATIONS,\n",
    "    END\n",
    ")\n",
    "\n",
    "# if the question is meant to be answered with a plot,\n",
    "# get the columns for the scatter plot\n",
    "workflow.add_node(\n",
    "    State.GET_SCATTER_PLOT_COLUMNS,\n",
    "    get_scatter_plot_columns,\n",
    ")\n",
    "workflow.add_node(\n",
    "    State.GENERATE_SCATTER_PLOT,\n",
    "    generate_scatter_plot,\n",
    ")\n",
    "workflow.add_edge(\n",
    "    State.GET_SCATTER_PLOT_COLUMNS,\n",
    "    State.GENERATE_SCATTER_PLOT\n",
    ")\n",
    "workflow.add_edge(\n",
    "    State.GENERATE_SCATTER_PLOT,\n",
    "    END\n",
    ")\n",
    "\n",
    "# if the question is not relevant, generate a funny response\n",
    "workflow.add_node(\n",
    "    State.GENERATE_FUNNY_RESPONSE,\n",
    "    generate_funny_response,\n",
    ")\n",
    "workflow.add_edge(\n",
    "    State.GENERATE_FUNNY_RESPONSE,\n",
    "    END\n",
    ")\n",
    "\n",
    "workflow.set_entry_point(State.CHECK_RELEVANCE)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"diagram.md\", \"w\") as f:\n",
    "    f.write(app.get_graph().draw_mermaid())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
